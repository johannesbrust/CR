
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>All-at-once optimization for CP tensor decomposition</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2022-08-16"><meta name="DC.source" content="cp_opt_doc.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:90%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000; line-height:140%; background:#fff none; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }
.banner{ background-color:#15243c; text-align:center;}
.navigate {font-size:0.8em; padding:0px; line-height:100%; }

pre, code { font-size:14px; }
tt { font-size: 1.0em; font-weight:bold; background:#f7f7f7; padding-right:5px; padding-left:5px }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:20px 0px 0px; border-top:1px dotted #878787; font-size:0.9em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; padding:0px 20px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="banner"><a href="index.html"><img src="Tensor-Toolbox-for-MATLAB-Banner.png"></a></div><div class="content"><h1>All-at-once optimization for CP tensor decomposition</h1><!--introduction--><p>
<p class="navigate">
&#62;&#62; <a href="index.html">Tensor Toolbox</a>
&#62;&#62; <a href="cp.html">CP Decompositions</a>
&#62;&#62; <a href="cp_opt_doc.html">CP-OPT</a>
</p>
</p><p>We explain how to use <tt>cp_opt</tt> function which implements the <b>CP-OPT</b> method that fits the CP model using <i>direct</i> or <i>all-at-once</i> optimization. This is in contrast to the <tt>cp_als</tt> function which implements the <b>CP-ALS</b> method that fits the CP model using <i>alternating</i> optimization. The CP-OPT method is described in the following reference:</p><div><ul><li>E. Acar, D. M. Dunlavy and T. G. Kolda, A Scalable Optimization Approach for Fitting Canonical Tensor Decompositions, J. Chemometrics, 25(2):67-86, 2011, <a href="http://doi.org/10.1002/cem.1335">http://doi.org/10.1002/cem.1335</a></li></ul></div><p>This method works with any tensor that support the functions <tt>size</tt>, <tt>norm</tt>, and <tt>mttkrp</tt>. This includes not only <tt>tensor</tt> and <tt>sptensor</tt>, but also <tt>ktensor</tt>, <tt>ttensor</tt>, and <tt>sumtensor</tt>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Major overhaul in Tensor Toolbox Version 3.3, August 2022</a></li><li><a href="#2">Optimization methods</a></li><li><a href="#3">Optimization parameters</a></li><li><a href="#4">Simple example problem #1</a></li><li><a href="#5">Calling <tt>cp_opt</tt> method and evaluating its outputs</a></li><li><a href="#9">Specifying different optimization method</a></li><li><a href="#11">Calling <tt>cp_opt</tt> method with higher rank</a></li><li><a href="#14">Simple example problem #2 (nonnegative)</a></li><li><a href="#15">Call the <tt>cp_opt</tt> method with lower bound of zero</a></li><li><a href="#16">Reproducibility</a></li><li><a href="#17">Specifying different optimization method</a></li></ul></div><h2 id="1">Major overhaul in Tensor Toolbox Version 3.3, August 2022</h2><p>The code was completely overhauled in the Version 3.3 release of the Tensor Toolbox. The old code is available in <tt>cp_opt_legacy</tt> and documented <a href="cp_opt_legacy_doc.html">here</a>. The major differences are as follows:</p><div><ol><li>The function being optimized is now <img src="cp_opt_doc_eq16322453171028127602.png" alt="$\|X - M\|^2 / \|X\|^2$" style="width:106px;height:16px;"> where <i>X</i> is the data tensor and <i>M</i> is the model. Previously, the function being optimized was <img src="cp_opt_doc_eq17622441662993199522.png" alt="$\|X-M\|^2/2$" style="width:80px;height:16px;">. The new formulation is only different by a constant factor, but its advantage is that the convergence tests (e.g., norm of gradient) are less sensitive to the scale of the <i>X</i>.</li><li>We now support the MATLAB Optimization Toolbox methods, <tt>fminunc</tt> and <tt>fmincon</tt>, the later of which has support for bound contraints.</li><li>The input and output arguments are different. We've retained the <a href="cp_opt_legacy_doc.html">legacy version</a> for those that cannot easily change their workflow.</li></ol></div><h2 id="2">Optimization methods</h2><p>The <tt>cp_opt</tt> methods uses optimization methods from other packages; see <a href="opt_options_doc.html">Optimization Methods for Tensor Toolbox</a>. As of Version 3.3., we distribute the default method (<tt>'lbfgsb'</tt>) along with Tensor Toolbox.</p><p>Options for 'method':</p><div><ul><li><tt>'lbfgsb'</tt> (default) - Uses <a href="https://github.com/stephenbeckr/L-BFGS-B-C"><b>L-BFGS-B</b> by Stephen Becker</a>, which implements the bound-constrained, limited-memory BFGS method. This code is distributed along with the Tensor Toolbox and will be activiated on first use. Supports bound contraints.</li><li><tt>'fminunc'</tt> or <tt>'fmincon'</tt> - Routines provided by the <b>MATLAB Optimization Toolbox</b>. The latter supports bound constraints. (It also supports linear and nonlinear constraints, but we have not included an interface to those.)</li><li><tt>'lbfgs'</tt> - Uses <a href="https://software.sandia.gov/trac/poblano"><b>POBLANO</b> Version 1.2 by Evrim Acar, Daniel Dunlavy, and Tamara Kolda</a>, which implemented the limited-memory BFGS method. Does not support bound constraints, but it is pure MATLAB and may work if <tt>'lbfgsb'</tt> does not.</li></ul></div><h2 id="3">Optimization parameters</h2><p>The full list of optimization parameters for each method are provide in <a href="opt_options_doc.html">Optimization Methods for Tensor Toolbox</a>. We list a few of the most relevant ones here.</p><div><ul><li><tt>'gtol'</tt> - The stopping condition for the norm of the gradient (or equivalent constrainted condition). Defaults to 1e-5.</li><li><tt>'lower'</tt> - Lower bounds, which can be a scalar (e.g., 0) or a vector. Defaults to <tt>-Inf</tt> (no lower bound).</li><li><tt>'printitn'</tt> - Frequency of printing. Normally, this specifies how often to print in terms of number of iteration. However, the MATLAB Optimization Toolbox method limit the options are 0 for none, 1 for every iteration, or &gt; 1 for just a final summary. These methods do not allow any more granularity than that.</li></ul></div><h2 id="4">Simple example problem #1</h2><p>Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.</p><pre class="codeinput">rng(1); <span class="comment">% Reproducibility</span>
R = 5;
problem = create_problem(<span class="string">'Size'</span>, [50 40 30], <span class="string">'Num_Factors'</span>, R, <span class="string">'Noise'</span>, 0.10);
X = problem.Data;
M_true = problem.Soln;

<span class="comment">% Create initial guess using 'nvecs'</span>
M_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R, <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><h2 id="5">Calling <tt>cp_opt</tt> method and evaluating its outputs</h2><p>Here is an example call to the cp_opt method. By default, each iteration prints the least squares fit function value (being minimized) and the norm of the gradient.</p><pre class="codeinput">[M,M0,info] = cp_opt(X, R, <span class="string">'init'</span>, M_init, <span class="string">'printitn'</span>, 25);
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: -Inf, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 5.529663e-02, ||grad||_infty = 2.18e-04
Iter    50, f(x) = 9.818485e-03, ||grad||_infty = 1.78e-04
Iter    55, f(x) = 9.809476e-03, ||grad||_infty = 3.71e-06
End Main Loop

Final f: 9.8095e-03
Setup time: 0.016 seconds
Optimization time: 0.33 seconds
Iterations: 55
Total iterations: 161
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.
</pre><p>It's important to check the output of the optimization method. In particular, it's worthwhile to check the exit message. The message <tt>CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH</tt> means that it has converged because the function value stopped improving. The message <tt>CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL</tt> means that the gradient is sufficiently small.</p><pre class="codeinput">exitmsg = info.optout.exit_condition
</pre><pre class="codeoutput">
exitmsg =

    'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.'

</pre><p>The objective function here is different than for CP-ALS. That uses the fit, which is <img src="cp_opt_doc_eq07029206319979379429.png" alt="$1 - (\|X-M\|/\|X\|)$" style="width:131px;height:15px;">. In other words, it is the percentage of the data that is explained by the model. Because we have 10% noise, we do not expect the fit to be about 90%.</p><pre class="codeinput">fit = 1 - sqrt(info.f)
</pre><pre class="codeoutput">
fit =

    0.9010

</pre><p>We can "score" the similarity of the model computed by CP and compare that with the truth. The <tt>score</tt> function on ktensor's gives a score in [0,1]  with 1 indicating a perfect match. Because we have noise, we do not expect the fit to be perfect. See <a href="matlab:doc('ktensor/score')">doc score</a> for more details.</p><pre class="codeinput">scr = score(M,M_true)
</pre><pre class="codeoutput">
scr =

    0.9986

</pre><h2 id="9">Specifying different optimization method</h2><pre class="codeinput">[M,M0,info] = cp_opt(X, R, <span class="string">'init'</span>, M_init, <span class="string">'printitn'</span>, 25, <span class="string">'method'</span>, <span class="string">'fminunc'</span>);
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
Unconstrained Optimization (via Optimization Toolbox)
Size: 50 x 40 x 30, Rank: 5
Parameters: gtol=1e-05, maxiters = 1000, maxsubiters=10000

Begin Main Loop

Local minimum found.

Optimization completed because the size of the gradient is less than
the value of the optimality tolerance.

End Main Loop

Final f: 9.8097e-03
Setup time: 1.7 seconds
Optimization time: 2.5 seconds
Iterations: 107
Total iterations: 1004
Exit condition: Gradient norm smaller than tolerance
</pre><p>Check exit condition</p><pre class="codeinput">exitmsg = info.optout.exit_condition
<span class="comment">% Fit</span>
fit = 1 - sqrt(info.f)
</pre><pre class="codeoutput">
exitmsg =

    'Gradient norm smaller than tolerance'


fit =

    0.9010

</pre><h2 id="11">Calling <tt>cp_opt</tt> method with higher rank</h2><p>Re-using the same example as before, consider the case where we don't know R in advance. We might guess too high. Here we show a case where we guess R+1 factors rather than R.</p><pre class="codeinput"><span class="comment">% Generate initial guess of the correct size</span>
rng(2);
M_plus_init = create_guess(<span class="string">'Data'</span>, X, <span class="string">'Num_Factors'</span>, R+1, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre><pre class="codeinput"><span class="comment">% Run the algorithm</span>
[M_plus,~,output] = cp_opt(X, R+1, <span class="string">'init'</span>, M_plus_init,<span class="string">'printitn'</span>,25);
exitmsg = info.optout.exit_condition
fit = 1 - sqrt(info.f)
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 6
Lower bound: -Inf, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 5.530646e-02, ||grad||_infty = 2.50e-04
Iter    50, f(x) = 9.803871e-03, ||grad||_infty = 8.52e-05
Iter    56, f(x) = 9.785758e-03, ||grad||_infty = 5.15e-06
End Main Loop

Final f: 9.7858e-03
Setup time: 0.0026 seconds
Optimization time: 0.16 seconds
Iterations: 56
Total iterations: 163
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.

exitmsg =

    'Gradient norm smaller than tolerance'


fit =

    0.9010

</pre><pre class="codeinput"><span class="comment">% Check the answer (1 is perfect)</span>
scr = score(M_plus, M_true)
</pre><pre class="codeoutput">
scr =

    0.9986

</pre><h2 id="14">Simple example problem #2 (nonnegative)</h2><p>We can employ lower bounds to get a nonnegative factorization. First, we create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise. We select nonnegative factor matrices and lambdas. The create_problem doesn't really know how to add noise without going negative, so we <i>hack</i> it to make the observed tensor be nonzero.</p><pre class="codeinput">R = 5;
rng(3);
problem2 = create_problem(<span class="string">'Size'</span>, [50 40 30], <span class="string">'Num_Factors'</span>, R, <span class="string">'Noise'</span>, 0.10,<span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'rand'</span>, <span class="string">'Lambda_Generator'</span>, <span class="string">'rand'</span>);
X = problem2.Data .* (problem2.Data &gt; 0); <span class="comment">% Force it to be nonnegative</span>
M_true = problem2.Soln;
</pre><h2 id="15">Call the <tt>cp_opt</tt> method with lower bound of zero</h2><p>Here we specify a lower bound of zero with the last two arguments.</p><pre class="codeinput">[M,M0,info] = cp_opt(X, R, <span class="string">'init'</span>, <span class="string">'rand'</span>,<span class="string">'lower'</span>,0,<span class="string">'printitn'</span>,25);

<span class="comment">% Check the output</span>
exitmsg = info.optout.exit_condition

<span class="comment">% Check the fit</span>
fit = 1 - sqrt(info.f)

<span class="comment">% Evaluate the output</span>
scr = score(M,M_true)
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: 0, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 1.534325e-02, ||grad||_infty = 1.80e-03
Iter    50, f(x) = 9.897878e-03, ||grad||_infty = 1.74e-04
Iter    75, f(x) = 9.786600e-03, ||grad||_infty = 5.02e-05
Iter    90, f(x) = 9.776759e-03, ||grad||_infty = 3.17e-05
End Main Loop

Final f: 9.7768e-03
Setup time: 0.0024 seconds
Optimization time: 0.13 seconds
Iterations: 90
Total iterations: 186
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.

exitmsg =

    'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.'


fit =

    0.9011


scr =

    0.9858

</pre><h2 id="16">Reproducibility</h2><p>The parameters of a run are saved, so that a run can be reproduced exactly as follows.</p><pre class="codeinput">cp_opt(X,R,info.params);
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: 0, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 1.534325e-02, ||grad||_infty = 1.80e-03
Iter    50, f(x) = 9.897878e-03, ||grad||_infty = 1.74e-04
Iter    75, f(x) = 9.786600e-03, ||grad||_infty = 5.02e-05
Iter    90, f(x) = 9.776759e-03, ||grad||_infty = 3.17e-05
End Main Loop

Final f: 9.7768e-03
Setup time: 0.0022 seconds
Optimization time: 0.13 seconds
Iterations: 90
Total iterations: 186
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.
</pre><h2 id="17">Specifying different optimization method</h2><pre class="codeinput">[M,M0,info] = cp_opt(X, R, <span class="string">'init'</span>, M_init, <span class="string">'printitn'</span>, 25, <span class="keyword">...</span>
    <span class="string">'method'</span>, <span class="string">'fmincon'</span>, <span class="string">'lower'</span>, 0);
</pre><pre class="codeoutput">
CP-OPT CP Direct Optimzation
Bound-Constrained Optimization (via Optimization Toolbox)
Size: 50 x 40 x 30, Rank: 5
Parameters: gtol=1e-05, maxiters = 1000, maxsubiters=10000

Begin Main Loop

Feasible point with lower objective function value found.


Local minimum found that satisfies the constraints.

Optimization completed because the objective function is non-decreasing in 
feasible directions, to within the value of the optimality tolerance,
and constraints are satisfied to within the value of the constraint tolerance.

End Main Loop

Final f: 1.1224e-01
Setup time: 0.059 seconds
Optimization time: 32 seconds
Iterations: 375
Total iterations: 1244
Exit condition: Gradient norm smaller than tolerance
</pre><p>Check exit condition</p><pre class="codeinput">exitmsg = info.optout.exit_condition
<span class="comment">% Fit</span>
fit = 1 - sqrt(info.f)
</pre><pre class="codeoutput">
exitmsg =

    'Gradient norm smaller than tolerance'


fit =

    0.6650

</pre><p class="footer">Tensor Toolbox for MATLAB: <a href="index.html">www.tensortoolbox.org</a>.</p></div><!--
##### SOURCE BEGIN #####
%% All-at-once optimization for CP tensor decomposition
%
% <html>
% <p class="navigate">
% &#62;&#62; <a href="index.html">Tensor Toolbox</a> 
% &#62;&#62; <a href="cp.html">CP Decompositions</a> 
% &#62;&#62; <a href="cp_opt_doc.html">CP-OPT</a>
% </p>
% </html>
%
% We explain how to use |cp_opt| function which implements the *CP-OPT*
% method that fits the CP model using _direct_ or _all-at-once_
% optimization. This is in contrast to the |cp_als| function which
% implements the *CP-ALS* method that fits the CP model using _alternating_ 
% optimization. The CP-OPT method is described in the
% following reference: 
%
% * E. Acar, D. M. Dunlavy and T. G. Kolda, A Scalable
% Optimization Approach for Fitting Canonical Tensor Decompositions,
% J. Chemometrics, 25(2):67-86, 2011,
% <http://doi.org/10.1002/cem.1335>
%
% This method works with any tensor that support the functions |size|,
% |norm|, and |mttkrp|. This includes not only |tensor| and |sptensor|, but
% also |ktensor|, |ttensor|, and |sumtensor|.

%% Major overhaul in Tensor Toolbox Version 3.3, August 2022
% The code was completely overhauled in the Version 3.3 release of the
% Tensor Toolbox. The old code is available in |cp_opt_legacy| and
% documented <cp_opt_legacy_doc.html here>. The major 
% differences are as follows:
% 
% # The function being optimized is now $\|X - M\|^2 / \|X\|^2$ where _X_
% is the data tensor and _M_ is the model. Previously, the function being
% optimized was $\|X-M\|^2/2$. The new formulation is only different by a
% constant factor, but its advantage is that the convergence tests (e.g.,
% norm of gradient) are less sensitive to the scale of the _X_. 
% # We now support the MATLAB Optimization Toolbox methods, |fminunc| and
% |fmincon|, the later of which has support for bound contraints.
% # The input and output arguments are different. 
% We've retained the <cp_opt_legacy_doc.html legacy version> for those that
% cannot easily change their workflow. 
% 

%% Optimization methods
% The |cp_opt| methods uses optimization methods from other packages; 
% see <opt_options_doc.html Optimization Methods for Tensor Toolbox>. 
% As of Version 3.3., we distribute the default method (|'lbfgsb'|)
% along with Tensor Toolbox.
%
% Options for 'method':
%
% * |'lbfgsb'| (default) - Uses 
% <https://github.com/stephenbeckr/L-BFGS-B-C *L-BFGS-B* by Stephen Becker>, 
% which implements the bound-constrained, limited-memory BFGS method. This
% code is distributed along with the Tensor Toolbox and will be activiated
% on first use. Supports bound contraints.
% * |'fminunc'| or |'fmincon'| - Routines provided by the *MATLAB Optimization
% Toolbox*. The latter supports bound constraints. (It also supports linear and nonlinear constraints, but we
% have not included an interface to those.)
% * |'lbfgs'| - Uses 
% <https://software.sandia.gov/trac/poblano *POBLANO* Version 1.2 by
% Evrim Acar, Daniel Dunlavy, and Tamara Kolda>, which implemented the
% limited-memory BFGS method. Does not support bound constraints, but it is
% pure MATLAB and may work if |'lbfgsb'| does not.
%

%% Optimization parameters
% The full list of optimization parameters for each method are provide in 
% <opt_options_doc.html Optimization Methods for Tensor Toolbox>.
% We list a few of the most relevant ones here.
%
% * |'gtol'| - The stopping condition for the norm of the gradient (or
% equivalent constrainted condition). Defaults to 1e-5.
% * |'lower'| - Lower bounds, which can be a scalar (e.g., 0) or a vector.
% Defaults to |-Inf| (no lower bound).
% * |'printitn'| - Frequency of printing. Normally, this specifies how 
% often to print in terms of number of iteration. 
% However, the MATLAB Optimization Toolbox method limit the options are 0
% for none, 1 for every iteration, or > 1 for just a final summary. These
% methods do not allow any more granularity than that.

%% Simple example problem #1
% Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.
rng(1); % Reproducibility
R = 5;
problem = create_problem('Size', [50 40 30], 'Num_Factors', R, 'Noise', 0.10);
X = problem.Data;
M_true = problem.Soln;

% Create initial guess using 'nvecs'
M_init = create_guess('Data', X, 'Num_Factors', R, 'Factor_Generator', 'nvecs');

%% Calling |cp_opt| method and evaluating its outputs
% Here is an example call to the cp_opt method. By default, each iteration
% prints the least squares fit function value (being minimized) and the
% norm of the gradient. 

[M,M0,info] = cp_opt(X, R, 'init', M_init, 'printitn', 25);

%% 
% It's important to check the output of the optimization method. In
% particular, it's worthwhile to check the exit message. 
% The message |CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH| means that
% it has converged because the function value stopped improving.
% The message |CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL| means that
% the gradient is sufficiently small.
exitmsg = info.optout.exit_condition

%% 
% The objective function here is different than for CP-ALS. That uses the
% fit, which is $1 - (\|X-M\|/\|X\|)$. In other words, it is the percentage
% of the data that is explained by the model. 
% Because we have 10% noise, we do not expect the fit to be about 90%.
fit = 1 - sqrt(info.f)

%% 
% We can "score" the similarity of the model computed by CP and compare
% that with the truth. The |score| function on ktensor's gives a score in
% [0,1]  with 1 indicating a perfect match. Because we have noise, we do
% not expect the fit to be perfect. See <matlab:doc('ktensor/score') doc
% score> for more details.
scr = score(M,M_true)

%% Specifying different optimization method
[M,M0,info] = cp_opt(X, R, 'init', M_init, 'printitn', 25, 'method', 'fminunc');

%%
% Check exit condition
exitmsg = info.optout.exit_condition
% Fit
fit = 1 - sqrt(info.f)

%% Calling |cp_opt| method with higher rank
% Re-using the same example as before, consider the case where we don't
% know R in advance. We might guess too high. Here we show a case where we
% guess R+1 factors rather than R. 

% Generate initial guess of the correct size
rng(2);
M_plus_init = create_guess('Data', X, 'Num_Factors', R+1, ...
    'Factor_Generator', 'nvecs');

%%

% Run the algorithm
[M_plus,~,output] = cp_opt(X, R+1, 'init', M_plus_init,'printitn',25);
exitmsg = info.optout.exit_condition
fit = 1 - sqrt(info.f)

%%

% Check the answer (1 is perfect)
scr = score(M_plus, M_true)


%% Simple example problem #2 (nonnegative)
% We can employ lower bounds to get a nonnegative factorization.
% First, we create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise. We
% select nonnegative factor matrices and lambdas. The
% create_problem doesn't really know how to add noise without going
% negative, so we _hack_ it to make the observed tensor be nonzero.
R = 5;
rng(3);
problem2 = create_problem('Size', [50 40 30], 'Num_Factors', R, 'Noise', 0.10,...
    'Factor_Generator', 'rand', 'Lambda_Generator', 'rand');
X = problem2.Data .* (problem2.Data > 0); % Force it to be nonnegative
M_true = problem2.Soln;

%% Call the |cp_opt| method with lower bound of zero
% Here we specify a lower bound of zero with the last two arguments.
[M,M0,info] = cp_opt(X, R, 'init', 'rand','lower',0,'printitn',25);

% Check the output
exitmsg = info.optout.exit_condition

% Check the fit
fit = 1 - sqrt(info.f)

% Evaluate the output
scr = score(M,M_true)

%% Reproducibility
% The parameters of a run are saved, so that a run can be reproduced
% exactly as follows.  
cp_opt(X,R,info.params);

%% Specifying different optimization method
[M,M0,info] = cp_opt(X, R, 'init', M_init, 'printitn', 25, ...
    'method', 'fmincon', 'lower', 0);

%%
% Check exit condition
exitmsg = info.optout.exit_condition
% Fit
fit = 1 - sqrt(info.f)
##### SOURCE END #####
--></body></html>