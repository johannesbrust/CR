
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Optimization Methods and Options for Tensor Toolbox</title><meta name="generator" content="MATLAB 9.11"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2022-08-31"><meta name="DC.source" content="opt_options_doc.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:90%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000; line-height:140%; background:#fff none; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }
.banner{ background-color:#15243c; text-align:center;}
.navigate {font-size:0.8em; padding:0px; line-height:100%; }

pre, code { font-size:14px; }
tt { font-size: 1.0em; font-weight:bold; background:#f7f7f7; padding-right:5px; padding-left:5px }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:20px 0px 0px; border-top:1px dotted #878787; font-size:0.9em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; padding:0px 20px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="banner"><a href="index.html"><img src="Tensor-Toolbox-for-MATLAB-Banner.png"></a></div><div class="content"><h1>Optimization Methods and Options for Tensor Toolbox</h1><!--introduction--><p>
<p class="navigate">
&#62;&#62; <a href="index.html">Tensor Toolbox</a>
&#62;&#62; <a href="opt_options_doc.html">Optimization Methods and Options</a>
</p>
</p><p><i>Within</i> the optimization-based functions in Tensor Toolbox like <a href="cp_opt_doc.html"><tt>cp_opt</tt></a>, we use a variety MATLAB optimization methods from different sources and with different interfaces. For some consisency, these methods are wrapped and given <i>standardized parameters</i> so that it is relatively easy to swap optimization methods. Here we describe the methods, the standardized parameters, installation instructions (if needed), and the mapping between the standardized parameters and the original ones.</p><div><ul><li>The optimization method is usually specified via the name-value pair   with name <tt>'method'</tt>.</li><li>The standardized parameters listed below are additional name-value   pairs that can be passed directly to the calling routine.</li><li>The standardized parameters marked with asterisks are not intended to be modified by the user but instead set by the calling routine, such as <tt>cp_opt</tt>.</li></ul></div><p>For more information on the details of these methods, see <a href="tt_opt_doc.html">Developer Information for Optimization Methods in Tensor Toolbox</a>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1"><tt>lbfgsb</tt>: Limited-Memory Quasi-Newton with Bound Constraints</a></li><li><a href="#2"><tt>lbfgs</tt>: Limited-Memory Quasi-Newton</a></li><li><a href="#3"><tt>fminunc</tt>: Optimizaton Toolbox Unconstrained Optimization</a></li><li><a href="#4"><tt>fmincon</tt>: Optimizaton Toolbox Constrained Optimization</a></li><li><a href="#5"><tt>adam</tt>: Stochastic Gradient Descent with Momentum</a></li></ul></div><h2 id="1"><tt>lbfgsb</tt>: Limited-Memory Quasi-Newton with Bound Constraints</h2><p><b>Source.</b> Included with Tensor Toolbox, adopted without any modification from <a href="https://github.com/stephenbeckr/L-BFGS-B-C"><b>L-BFGS-B</b> by Stephen Becker</a>.</p><p><b>Standardized Parameters.</b></p><p>
<table>
<tr><td>Name</td><td>Description</td><td>Default</td></tr>
<tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
<tr><td><tt>upper</tt></td><td>Upper bounds, can be vector or scalar</td><td><tt>+Inf</tt></td></tr>
<tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
<tr><td><tt>printitn</tt></td><td>Printing frequency by iteration (0=no output)</td><td>1</td></tr>
<tr><td><tt>m</tt></td><td>Limited-memory parameter</td><td>5</td></tr>
<tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
<tr><td><tt>ftol</tt></td><td>Stopping condition based on relative function change</td><td>1e-10</td></tr>
<tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
<tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'L-BFGS-B Optimization'</tt></td></tr>
<tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
</table>
</p><p><b>Installation Instructions.</b> Nothing should be required as this code is contained in <tt>libraries/lbfgs</tt>. However, if needed, please see the <a href="https://github.com/stephenbeckr/L-BFGS-B-C">GitHub site</a> for full details on references, installation, etc. Here we provide cursory instructions for installation:</p><div><ol><li>Download the zip file <a href="https://github.com/stephenbeckr/L-BFGS-B-C/archive/master.zip">&lt;https://github.com/stephenbeckr/L-BFGS-B-C/archive/master.zip</a>&gt;</li><li>Unzip and goto the <tt>Matlab/</tt> subdirectoy with MATLAB</li><li>Type <tt>compile_mex</tt></li><li><i>Add this directory to your saved path!</i></li></ol></div><p><b>Mapping of Standardized parameters.</b> The wrapper for this method is <tt>tt_opt_lbfgsb</tt> in the Tensor Toolbox. Notes regarding mappings to the parameters of Becker's L-BFGS-B code:</p><div><ul><li><tt>maxIts</tt> maps to <tt>maxiters</tt> and the default is increased from 100 to 1000</li><li><tt>printEvery</tt> maps to <tt>printitn</tt></li><li><tt>maxTotalIts</tt> is set to <tt>maxiters*subiters</tt> and this effectively changes the default from 5000 to 10000</li><li><tt>factr</tt> is set to <tt>ftol</tt> / eps and this effectively changes the default from 1e7 to 4.5e5</li><li><tt>pgtol</tt> is set to <tt>gtol</tt></li></ul></div><h2 id="2"><tt>lbfgs</tt>: Limited-Memory Quasi-Newton</h2><p><b>Source.</b> <tt>lbfgs</tt> method in <a href="https://github.com/sandialabs/poblano_toolbox/releases/tag/v1.2"><b>Poblano Toolbox</b>, v1.2</a>.</p><p><b>Standardized Parameters.</b></p><p>
<table>
<tr><td>Name</td><td>Description</td><td>Default</td></tr>
<tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
<tr><td><tt>printitn</tt></td><td>Printing frequency by iteration (0=no output)</td><td>1</td></tr>
<tr><td><tt>m</tt></td><td>Limited-memory parameter</td><td>5</td></tr>
<tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
<tr><td><tt>ftol</tt></td><td>Stopping condition based on relative function change</td><td>1e-10</td></tr>
<tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
<tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Poblano L-BFGS Optimization'</tt></td></tr>
<tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
</table>
</p><p><b>Installation Instructions.</b> Download and install <a href="https://github.com/sandialabs/poblano_toolbox/releases/tag/v1.2"><b>Poblano Toolbox</b>, v1.2</a>. Please see that web page for full details on references, installation, etc. Here we provide cursory instructions for installation:</p><div><ol><li>Download the zip file <a href="https://github.com/sandialabs/poblano_toolbox/archive/v1.2.zip">&lt;https://github.com/sandialabs/poblano_toolbox/archive/v1.2.zip</a>&gt;</li><li>Unzip and goto the <tt>poblano_toolbox-1.2/</tt> subdirectoy within MATLAB</li><li>Type <tt>install_poblano</tt> to save this directory to your path</li></ol></div><p><b>Mapping of Standardized parameters.</b> The wrapper for this method is <tt>tt_opt_lbfgs</tt> in the Tensor Toolbox. Notes regarding mappings to the parameters of Poblano's L-BFGS code:</p><div><ul><li><tt>MaxIters</tt> maps to <tt>maxiters</tt></li><li><tt>Display</tt> maps to <tt>printitn</tt></li><li><tt>MaxFuncEvals</tt> is set to <tt>maxiters*subiters</tt></li><li><tt>RelFuncTol</tt> is set to <tt>ftol</tt></li><li><tt>StopTol</tt> is set to <tt>gtol</tt></li></ul></div><h2 id="3"><tt>fminunc</tt>: Optimizaton Toolbox Unconstrained Optimization</h2><p><b>Source.</b> MATLAB Optimization Toolbox.</p><p><b>Standardized Parameters.</b></p><p>
<table>
<tr><td>Name</td><td>Description</td><td>Default</td></tr>
<tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
<tr><td><tt>printitn</tt></td><td>Display (0=no output)</td><td>1</td></tr>
<tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
<tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
<tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Unconstrained Optimization (via Optimization Toolbox)'</tt></td></tr>
<tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
</table>
</p><p><b>Installation Instructions.</b> Install MATLAB Optimization Toolbox.</p><p><b>Mapping of Standardized parameters.</b></p><div><ul><li><tt>MaxFunctionEvaluations</tt> is set to <tt>maxiters*subiters</tt></li><li><tt>MaxIterations</tt> is set to <tt>maxiters</tt></li><li><tt>OptimalityTolerance</tt> is set to <tt>gtol</tt></li><li><tt>Display</tt> is set of <tt>'off'</tt> if <tt>printitn</tt> = 0, <tt>'iter'</tt> if <tt>printitn</tt> = 1, and <tt>'final'</tt> if <tt>printitn</tt> &gt; 1.</li></ul></div><h2 id="4"><tt>fmincon</tt>: Optimizaton Toolbox Constrained Optimization</h2><p><b>Source.</b> MATLAB Optimization Toolbox.</p><p><b>Standardized Parameters.</b> Same as for <tt>fminunc</tt> plus the following:</p><p>
<table>
<tr><td>Name</td><td>Description</td><td>Default</td></tr>
<tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
<tr><td><tt>upper</tt></td><td>Upper bounds, can be vector or scalar</td><td><tt>+Inf</tt></td></tr>
<tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Constrained Optimization (via Optimization Toolbox)'</tt></td></tr>
</table>
</p><p><b>Installation Instructions.</b> Install MATLAB Optimization Toolbox.</p><p><b>Mapping of Standardized parameters.</b></p><div><ul><li>The lower bound (7th input to <tt>fmincon</tt>) is set to <tt>[]</tt> if <tt>lower</tt> = <tt>Inf</tt>. If it's a scalar, then the lower bound is set to a vector of appropriate length with the scalar in every position. Otherwise, it's set to the provided vector.</li><li>The upper bound (8th input) is analogous.</li></ul></div><h2 id="5"><tt>adam</tt>: Stochastic Gradient Descent with Momentum</h2><p><b>Source.</b> This is our own implementation of Adam. A <i>failed epoch</i> is one where the function value does not decrease. After a failed epoch, the method either reduces the learning rate (by <tt>decay</tt>) or exits (once the number of failed epochs exceeds <tt>maxfails</tt>).</p><p><b>Standardized Parameters.</b></p><p>
<table>
<tr><td>Name</td><td>Description</td><td>Default</td></tr>
<tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
<tr><td><tt>subiters</tt></td><td>Number of iterations per epoch</td><td>100</td></tr>
<tr><td><tt>maxiters</tt></td><td>Maximum number of epochs</td><td>100</td></tr>
<tr><td><tt>rate</tt></td><td>Initial learning rate</td><td>1e-2</td></tr>
<tr><td><tt>maxfails</tt></td><td>Maximum number of failed epochs</td><td>1</td></tr>
<tr><td><tt>decay</tt></td><td>Decay of learning rate after failed epoch</td><td>0.1</td></tr>
<tr><td><tt>backup</tt></td><td>Revert to end of previous epoch after failure</td><td>true</td></tr>
<tr><td><tt>ftol</tt></td><td>Quit if function value goes below this value</td><td><tt>-Inf</tt></td></tr>
<tr><td><tt>beta1</tt></td><td>Adam parameter</td><td>0.9</td></tr>
<tr><td><tt>beta2</tt></td><td>Adam parameter</td><td>0.999</td></tr>
<tr><td><tt>epsilon</tt></td><td>Adam parameter</td><td>1e-8</td></tr>
<tr><td><tt>printitn</tt></td><td>Printing frequency by epoch (0=no output)</td><td>1</td></tr>
<tr><td><tt>state</tt> (*)</td><td>State of random number generator</td><td>current state</td></tr>
<tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Adam Stochastic Optimization'</tt></td></tr>
<tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
<tr><td><tt>fdesc</tt> (*)</td><td>Description of (approximate) function computation</td><td>none</td></tr>
<tr><td><tt>gdesc</tt> (*)</td><td>Description of stochastic gradient computation</td><td>none</td></tr>
<tr><td><tt>fexact</tt> (*)</td><td>Boolean if function is computed exactly</td><td>true</td></tr>
</table>
</p><p class="footer">Tensor Toolbox for MATLAB: <a href="index.html">www.tensortoolbox.org</a>.</p></div><!--
##### SOURCE BEGIN #####
%% Optimization Methods and Options for Tensor Toolbox
%
% <html>
% <p class="navigate">
% &#62;&#62; <a href="index.html">Tensor Toolbox</a> 
% &#62;&#62; <a href="opt_options_doc.html">Optimization Methods and Options</a> 
% </p>
% </html>
%
% _Within_ the optimization-based functions in Tensor Toolbox like <cp_opt_doc.html |cp_opt|>, 
% we use a variety MATLAB optimization methods from different sources and
% with different interfaces. For some consisency, these methods are 
% wrapped and given _standardized parameters_ so that it is relatively easy to
% swap optimization methods. 
% Here we describe the methods, the standardized parameters, installation
% instructions (if needed), and the mapping between the standardized
% parameters and the original ones.
%
% * The optimization method is usually specified via the name-value pair
%   with name |'method'|. 
% * The standardized parameters listed below are additional name-value
%   pairs that can be passed directly to the calling routine.
% * The standardized parameters marked with asterisks are not intended to
% be modified by the user but instead set by the calling routine, such as
% |cp_opt|. 
%
% For more information on the details of these methods, see
% <tt_opt_doc.html Developer Information for Optimization Methods in Tensor Toolbox>.

%% |lbfgsb|: Limited-Memory Quasi-Newton with Bound Constraints 
% *Source.*
% Included with Tensor Toolbox, adopted without any modification from
% <https://github.com/stephenbeckr/L-BFGS-B-C *L-BFGS-B* by Stephen
% Becker>.
%
% *Standardized Parameters.* 
%
% <html>
% <table>
% <tr><td>Name</td><td>Description</td><td>Default</td></tr>
% <tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
% <tr><td><tt>upper</tt></td><td>Upper bounds, can be vector or scalar</td><td><tt>+Inf</tt></td></tr>
% <tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
% <tr><td><tt>printitn</tt></td><td>Printing frequency by iteration (0=no output)</td><td>1</td></tr>
% <tr><td><tt>m</tt></td><td>Limited-memory parameter</td><td>5</td></tr>
% <tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
% <tr><td><tt>ftol</tt></td><td>Stopping condition based on relative function change</td><td>1e-10</td></tr>
% <tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
% <tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'L-BFGS-B Optimization'</tt></td></tr>
% <tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
% </table>
% </html>
% 
% *Installation Instructions.*
% Nothing should be required as this code is contained in
% |libraries/lbfgs|. However, if needed, please see the
% <https://github.com/stephenbeckr/L-BFGS-B-C GitHub site>   
% for full details on references, installation, etc. 
% Here we provide cursory instructions for installation:
% 
% # Download the zip file <https://github.com/stephenbeckr/L-BFGS-B-C/archive/master.zip https://github.com/stephenbeckr/L-BFGS-B-C/archive/master.zip>
% # Unzip and goto the |Matlab/| subdirectoy with MATLAB
% # Type |compile_mex|
% # _Add this directory to your saved path!_
%
% *Mapping of Standardized parameters.*
% The wrapper for this method is |tt_opt_lbfgsb| in the Tensor Toolbox. 
% Notes regarding mappings to the parameters of Becker's L-BFGS-B code:
% 
% * |maxIts| maps to |maxiters| and the default is increased from 100 to 1000
% * |printEvery| maps to |printitn|
% * |maxTotalIts| is set to |maxiters*subiters| and this effectively changes the default from 5000 to 10000
% * |factr| is set to |ftol| / eps and this effectively changes the default from 1e7 to 4.5e5
% * |pgtol| is set to |gtol| 

%% |lbfgs|: Limited-Memory Quasi-Newton 
% *Source.* |lbfgs| method in
% <https://github.com/sandialabs/poblano_toolbox/releases/tag/v1.2 *Poblano
% Toolbox*, v1.2>.     
%
% *Standardized Parameters.* 
%
% <html>
% <table>
% <tr><td>Name</td><td>Description</td><td>Default</td></tr>
% <tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
% <tr><td><tt>printitn</tt></td><td>Printing frequency by iteration (0=no output)</td><td>1</td></tr>
% <tr><td><tt>m</tt></td><td>Limited-memory parameter</td><td>5</td></tr>
% <tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
% <tr><td><tt>ftol</tt></td><td>Stopping condition based on relative function change</td><td>1e-10</td></tr>
% <tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
% <tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Poblano L-BFGS Optimization'</tt></td></tr>
% <tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
% </table>
% </html>
% 
% *Installation Instructions.*
% Download and install
% <https://github.com/sandialabs/poblano_toolbox/releases/tag/v1.2 *Poblano Toolbox*, v1.2>.   
% Please see that web page for full details on references, installation, etc. 
% Here we provide cursory instructions for installation:
% 
% # Download the zip file <https://github.com/sandialabs/poblano_toolbox/archive/v1.2.zip https://github.com/sandialabs/poblano_toolbox/archive/v1.2.zip>
% # Unzip and goto the |poblano_toolbox-1.2/| subdirectoy within MATLAB
% # Type |install_poblano| to save this directory to your path
%
% *Mapping of Standardized parameters.*
% The wrapper for this method is |tt_opt_lbfgs| in the Tensor Toolbox. 
% Notes regarding mappings to the parameters of Poblano's L-BFGS code:
% 
% * |MaxIters| maps to |maxiters|
% * |Display| maps to |printitn|
% * |MaxFuncEvals| is set to |maxiters*subiters| 
% * |RelFuncTol| is set to |ftol| 
% * |StopTol| is set to |gtol| 

%% |fminunc|: Optimizaton Toolbox Unconstrained Optimization
% *Source.*
% MATLAB Optimization Toolbox.
%
% *Standardized Parameters.* 
%
% <html>
% <table>
% <tr><td>Name</td><td>Description</td><td>Default</td></tr>
% <tr><td><tt>maxiters</tt></td><td>Max outer iterations</td><td>1000</td></tr>
% <tr><td><tt>printitn</tt></td><td>Display (0=no output)</td><td>1</td></tr>
% <tr><td><tt>subiters</tt></td><td>Controls maximum calls to function-gradient evalations</td><td>10</td></tr>
% <tr><td><tt>gtol</tt></td><td>Stopping condition based on gradient norm</td><td>1e-5</td></tr>
% <tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Unconstrained Optimization (via Optimization Toolbox)'</tt></td></tr>
% <tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
% </table>
% </html>
%
% *Installation Instructions.*
% Install MATLAB Optimization Toolbox.
%
% *Mapping of Standardized parameters.*
%
% * |MaxFunctionEvaluations| is set to |maxiters*subiters|
% * |MaxIterations| is set to |maxiters|
% * |OptimalityTolerance| is set to |gtol|
% * |Display| is set of |'off'| if |printitn| = 0, |'iter'| if
% |printitn| = 1, and |'final'| if |printitn| > 1.
%% |fmincon|: Optimizaton Toolbox Constrained Optimization
% *Source.*
% MATLAB Optimization Toolbox.
%
% *Standardized Parameters.* 
% Same as for |fminunc| plus the following:
%
% <html>
% <table>
% <tr><td>Name</td><td>Description</td><td>Default</td></tr>
% <tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
% <tr><td><tt>upper</tt></td><td>Upper bounds, can be vector or scalar</td><td><tt>+Inf</tt></td></tr>
% <tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Constrained Optimization (via Optimization Toolbox)'</tt></td></tr>
% </table>
% </html>
%
% *Installation Instructions.*
% Install MATLAB Optimization Toolbox.
%
% *Mapping of Standardized parameters.*
%
% * The lower bound (7th input to |fmincon|) is set to |[]| if |lower| =
% |Inf|. If it's a scalar, then the lower bound is set to a vector of
% appropriate length with the scalar in every position. Otherwise, it's set
% to the provided vector.
% * The upper bound (8th input) is analogous.

%% |adam|: Stochastic Gradient Descent with Momentum
% *Source.*
% This is our own implementation of Adam. A _failed epoch_ is one where the
% function value does not decrease. After a failed epoch, the method either
% reduces the learning rate (by |decay|) or exits (once the number of
% failed epochs exceeds |maxfails|). 
%
%
% *Standardized Parameters.* 
%
% <html>
% <table>
% <tr><td>Name</td><td>Description</td><td>Default</td></tr>
% <tr><td><tt>lower</tt></td><td>Lower bounds, can be vector or scalar</td><td><tt>-Inf</tt></td></tr>
% <tr><td><tt>subiters</tt></td><td>Number of iterations per epoch</td><td>100</td></tr>
% <tr><td><tt>maxiters</tt></td><td>Maximum number of epochs</td><td>100</td></tr>
% <tr><td><tt>rate</tt></td><td>Initial learning rate</td><td>1e-2</td></tr>
% <tr><td><tt>maxfails</tt></td><td>Maximum number of failed epochs</td><td>1</td></tr>
% <tr><td><tt>decay</tt></td><td>Decay of learning rate after failed epoch</td><td>0.1</td></tr>
% <tr><td><tt>backup</tt></td><td>Revert to end of previous epoch after failure</td><td>true</td></tr>
% <tr><td><tt>ftol</tt></td><td>Quit if function value goes below this value</td><td><tt>-Inf</tt></td></tr>
% <tr><td><tt>beta1</tt></td><td>Adam parameter</td><td>0.9</td></tr>
% <tr><td><tt>beta2</tt></td><td>Adam parameter</td><td>0.999</td></tr>
% <tr><td><tt>epsilon</tt></td><td>Adam parameter</td><td>1e-8</td></tr>
% <tr><td><tt>printitn</tt></td><td>Printing frequency by epoch (0=no output)</td><td>1</td></tr>
% <tr><td><tt>state</tt> (*)</td><td>State of random number generator</td><td>current state</td></tr>
% <tr><td><tt>mdesc</tt> (*)</td><td>Method description printed out before run</td><td><tt>'Adam Stochastic Optimization'</tt></td></tr>
% <tr><td><tt>xdesc</tt> (*)</td><td>Variable description</td><td>auto-generated</td></tr>
% <tr><td><tt>fdesc</tt> (*)</td><td>Description of (approximate) function computation</td><td>none</td></tr>
% <tr><td><tt>gdesc</tt> (*)</td><td>Description of stochastic gradient computation</td><td>none</td></tr>
% <tr><td><tt>fexact</tt> (*)</td><td>Boolean if function is computed exactly</td><td>true</td></tr>
% </table>
% </html>
% 




##### SOURCE END #####
--></body></html>